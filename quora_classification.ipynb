{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from torch import optim\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from torch.nn import functional as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_ensemble = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag = True\n",
    "if flag:\n",
    "    train_file = 'data/train.csv'\n",
    "    embedding_file = 'data/glove.840B.300d.txt'\n",
    "    test_file = 'data/test.csv'\n",
    "else:\n",
    "    train_file = '../input/train.csv'\n",
    "    embedding_file = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    test_file = '../input/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "batch_size = 256\n",
    "test_batch_size = 1024\n",
    "epochs = 1\n",
    "LR = 5e-3\n",
    "lr_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_model_rnn_1 = {\n",
    "    'mode': 'LSTM',\n",
    "    'hidden_size': 150,\n",
    "    'dropout_p': 0.2,\n",
    "    'encoder_dropout_p': 0.1,\n",
    "    'encoder_layer_num': 1,\n",
    "    'is_bn': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_model_rnn_2 = {\n",
    "    'mode': 'GRU',\n",
    "    'hidden_size': 150,\n",
    "    'dropout_p': 0.2,\n",
    "    'encoder_dropout_p': 0.1,\n",
    "    'encoder_layer_num': 1,\n",
    "    'is_bn': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    " - 分词\n",
    " - train：长度筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "specials_d = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "specials_c = {'\\u200b': ' ','…': ' ... ','\\ufeff': '','करना': '','है': ''}\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_data(sentence_list):\n",
    "\n",
    "    result = []\n",
    "    for i in sentence_list:\n",
    "        i = re.sub(r'\\s+', ' ', i)\n",
    "\n",
    "        # 缩写词替换\n",
    "        for s in specials_d:\n",
    "            i = i.replace(s, \"'\")\n",
    "        for word in contraction_mapping.keys():\n",
    "            i = i.replace(word, contraction_mapping[word])\n",
    "\n",
    "        # 错误词替换\n",
    "        for s in specials_c:\n",
    "            i = i.replace(s, specials_c[s])\n",
    "        for word in mispell_dict.keys():\n",
    "            i = i.replace(word, mispell_dict[word])\n",
    "\n",
    "        i = re.sub(r'\\s+', ' ', i)\n",
    "        i = nltk.word_tokenize(i)\n",
    "\n",
    "        result.append(i)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deal_data(data, max_len=100, is_train=True):\n",
    "    df = pd.read_csv(data)\n",
    "    questions = df['question_text'].values\n",
    "    question_word_lists = pre_data(questions)\n",
    "    question_word_list_len = [len(q) for q in question_word_lists]\n",
    "    if is_train:\n",
    "        target = df['target'].values\n",
    "        question_os = []\n",
    "        target_os = []\n",
    "        for q, t, l in zip(question_word_lists, target, question_word_list_len):\n",
    "            if l <= max_len:\n",
    "                question_os.append(q)\n",
    "                target_os.append(t)\n",
    "        print('deal_data, retain data:%d/%d' % (len(question_os), len(questions)))\n",
    "        return question_os, target_os\n",
    "\n",
    "    else:\n",
    "        question_os = question_word_lists\n",
    "        return question_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal_data, retain data:1306111/1306122\n",
      "train_len:1306111\n",
      "test_len:56370\n"
     ]
    }
   ],
   "source": [
    "train_questions, train_targets = deal_data(train_file, max_len=max_len)\n",
    "print('train_len:%d' % (len(train_questions)))\n",
    "test_questions = deal_data(test_file, max_len=max_len, is_train=False)\n",
    "print('test_len:%d' % (len(test_questions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fake\n",
    "# train_questions, train_targets = train_questions[: 1000], train_targets[: 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立词表\n",
    " - glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_embedding(questions, glove_file):\n",
    "\n",
    "    # 初始化embedding字典\n",
    "    def get_matrixs(word, *nums):\n",
    "        return word, np.asarray(nums, dtype='float32')\n",
    "    embedding_dict = dict([get_matrixs(*o.split(' ')) for o in open(glove_file, 'r')])\n",
    "\n",
    "    # 初始化词表\n",
    "    word_set = set()\n",
    "    for q in questions:\n",
    "        for word in q:\n",
    "            word_set.add(word)\n",
    "    vocab_all_size = len(word_set)\n",
    "\n",
    "    # 词表删选\n",
    "    word_set = set()\n",
    "    for q in questions:\n",
    "        for word in q:\n",
    "            if word in embedding_dict:\n",
    "                word_set.add(word)\n",
    "    vocab_size = len(word_set)\n",
    "\n",
    "    print('words in pre-embedding, num:%d/%d, radio:%.4f' % (vocab_size, vocab_all_size, vocab_size/vocab_all_size))\n",
    "\n",
    "    # 构建词表、embedding矩阵\n",
    "    w2i = {'<pad>': 0}\n",
    "    count = 1\n",
    "    embedding = np.zeros([len(word_set)+2, 300])\n",
    "    for word in word_set:\n",
    "        if word not in w2i:\n",
    "            w2i[word] = count\n",
    "            embedding[count] = embedding_dict[word]\n",
    "            count += 1\n",
    "    w2i['<unk>'] = count\n",
    "    assert len(w2i) == len(embedding)\n",
    "\n",
    "    print('build_word_embedding,  vocab size:%d' % len(w2i))\n",
    "\n",
    "    return w2i, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_embedding(questions, glove_file):\n",
    "\n",
    "    # 初始化embedding字典\n",
    "    def get_matrixs(word, *nums):\n",
    "        return word, np.asarray(nums, dtype='float32')\n",
    "    embedding_dict = dict([get_matrixs(*o.split(' ')) for o in open(glove_file, 'r')])\n",
    "\n",
    "    # 初始化词表\n",
    "    vocab = {}\n",
    "    for q in questions:\n",
    "        for word in q:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "\n",
    "    # 检查覆盖率、词表删选\n",
    "    known_num = 0\n",
    "    all_num = 0\n",
    "    word_set = []\n",
    "    for word in vocab.keys():\n",
    "        if word in embedding_dict:\n",
    "            known_num += vocab[word]\n",
    "            word_set.append(word)\n",
    "        all_num += vocab[word]\n",
    "\n",
    "    print('words in pre-embedding, num:%d/%d, radio:%.4f' % (len(word_set), len(vocab), len(word_set)/len(vocab)))\n",
    "    print('known words in all text:%.4f' % (known_num/all_num))\n",
    "\n",
    "    # 构建词表、embedding矩阵\n",
    "    w2i = {'<pad>': 0}\n",
    "    count = 1\n",
    "    embedding = np.zeros([len(word_set)+2, 300])\n",
    "    for word in word_set:\n",
    "        if word not in w2i:\n",
    "            w2i[word] = count\n",
    "            embedding[count] = embedding_dict[word]\n",
    "            count += 1\n",
    "    w2i['<unk>'] = count\n",
    "    assert len(w2i) == len(embedding)\n",
    "\n",
    "    print('build_word_embedding,  vocab size:%d' % len(w2i))\n",
    "\n",
    "    return w2i, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in pre-embedding, num:200419/313313, radio:0.6397\n",
      "known words in all text:0.9923\n",
      "build_word_embedding,  vocab size:200421\n"
     ]
    }
   ],
   "source": [
    "w2i, embedding = build_word_embedding(train_questions+test_questions, embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2indexs(words, lang):\n",
    "\n",
    "    def word2index(word_list):\n",
    "        return [lang[word] if word in lang else lang['<unk>'] for word in word_list]\n",
    "\n",
    "    return [word2index(word_list) for word_list in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_questions = word2indexs(train_questions, w2i)\n",
    "test_questions = word2indexs(test_questions, w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padding(words, max_len, pad_index=0):\n",
    "\n",
    "    def padd(word_list):\n",
    "        if len(word_list) > max_len:\n",
    "            tmp = word_list[: max_len]\n",
    "        else:\n",
    "            tmp = word_list + [pad_index] * (max_len - len(word_list))\n",
    "        return tmp\n",
    "\n",
    "    results = [padd(word_list) for word_list in words]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_questions = padding(train_questions, max_len)\n",
    "test_questions = padding(test_questions, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机划分训练集、验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len:1175499, val_len:130612\n"
     ]
    }
   ],
   "source": [
    "train_questions, val_questions, train_targets, val_targets = model_selection.train_test_split(\n",
    "        train_questions, train_targets, test_size=0.1, random_state=333)\n",
    "assert len(train_questions) == len(train_targets)\n",
    "assert len(val_questions) == len(val_targets)\n",
    "print('train_len:%d, val_len:%d' % (len(train_questions), len(val_questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建train、val dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size, shuffle, drop_last):\n",
    "    dataset = [torch.LongTensor(d) for d in dataset]\n",
    "    dataset = data.TensorDataset(*dataset)\n",
    "    data_iter = data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last\n",
    "    )\n",
    "    return data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(\n",
    "    dataset=[train_questions, train_targets],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = get_dataloader(\n",
    "    dataset=[val_questions, val_targets],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = get_dataloader(\n",
    "    dataset=[test_questions],\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding\n",
    " - 基础embedding\n",
    " - <unk> 可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\" standard embedding \"\"\"\n",
    "    def __init__(self, embedding):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.vocab_size = embedding.shape[0]\n",
    "        self.w2v_size = embedding.shape[1]\n",
    "\n",
    "        self.embedding_fix = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.w2v_size,\n",
    "            padding_idx=0,\n",
    "            _weight=torch.Tensor(embedding)\n",
    "        )\n",
    "        self.embedding_fix.weight.requires_grad = False\n",
    "\n",
    "        self.embedding_v = nn.Embedding(\n",
    "            num_embeddings=2,\n",
    "            embedding_dim=self.w2v_size,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.embedding_dim = self.embedding_fix.embedding_dim\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: (batch_size, c_len)\n",
    "        :return: (batch_size, c_len, w2v)\n",
    "        \"\"\"\n",
    "        embedding_1 = self.embedding_fix(tensor)\n",
    "\n",
    "        tensor = tensor - (self.vocab_size - self.embedding_v.num_embeddings)\n",
    "        tensor = f.relu(tensor)\n",
    "        embedding_2 = self.embedding_v(tensor)\n",
    "\n",
    "        embedding = embedding_1 + embedding_2\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder\n",
    " - LSTM、 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rnn(nn.Module):\n",
    "\n",
    "    def __init__(self, param):\n",
    "        super(Rnn, self).__init__()\n",
    "\n",
    "        self.mode = param['mode']\n",
    "        self.input_size = param['input_size']\n",
    "        self.hidden_size = param['hidden_size']\n",
    "        self.dropout_p = param['encoder_dropout_p']\n",
    "        self.directional = True\n",
    "        self.layer_num = param['encoder_layer_num']\n",
    "        self.is_bn = param['is_bn']\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.layer_num,\n",
    "                bidirectional=self.directional,\n",
    "                dropout=self.dropout_p if self.layer_num > 1 else 0\n",
    "            )\n",
    "        elif self.mode == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=self.input_size,\n",
    "                hidden_size=self.hidden_size,\n",
    "                num_layers=self.layer_num,\n",
    "                bidirectional=self.directional,\n",
    "                dropout=self.dropout_p if self.layer_num > 1 else 0\n",
    "            )\n",
    "\n",
    "        if self.is_bn:\n",
    "            self.layer_norm = nn.LayerNorm(self.input_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\" use xavier_uniform to initialize rnn weights \"\"\"\n",
    "        ih = (param for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param for name, param in self.named_parameters() if 'bias' in name)\n",
    "\n",
    "        for t in ih:\n",
    "            torch.nn.init.xavier_uniform_(t)\n",
    "        for t in hh:\n",
    "            torch.nn.init.orthogonal_(t)\n",
    "        for t in b:\n",
    "            torch.nn.init.constant_(t, 0)\n",
    "\n",
    "    def forward(self, vec, mask):\n",
    "        \"\"\"\n",
    "        :param vec: (seq_len, batch_size, input_size)\n",
    "        :param mask: (batch_size, seq_len)\n",
    "        :return: (seq_len, batch_size, hidden_size*directional_num)\n",
    "        \"\"\"\n",
    "\n",
    "        # layer normalization\n",
    "        if self.is_bn:\n",
    "            seq_len, batch_size, input_size = vec.size\n",
    "            vec = vec.contiguous().view(-1, input_size)\n",
    "            vec = self.layer_norm(vec)\n",
    "            vec = vec.view(seq_len, batch_size, input_size)\n",
    "\n",
    "        # dropout\n",
    "        vec = self.dropout(vec)\n",
    "\n",
    "        # forward\n",
    "        lengths = mask.long().sum(1)\n",
    "        length_sort, idx_sort = torch.sort(lengths, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort)\n",
    "\n",
    "        v_sort = vec.index_select(1, idx_sort)\n",
    "        v_pack = nn.utils.rnn.pack_padded_sequence(v_sort, length_sort)\n",
    "        outputs, _ = self.rnn(v_pack, None)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs.index_select(1, idx_unsort)\n",
    "\n",
    "        # 未填充， outputs的第一维可能小于seq_len\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model：Bi-Rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_Rnn(nn.Module):\n",
    "    \"\"\" rnn \"\"\"\n",
    "    def __init__(self, param):\n",
    "        super(Model_Rnn, self).__init__()\n",
    "\n",
    "        self.hidden_size = param['hidden_size']\n",
    "        self.dropout_p = param['dropout_p']\n",
    "\n",
    "        # embedding\n",
    "        self.embedding = Embedding(param['embedding'])\n",
    "\n",
    "        # encoder\n",
    "        param['input_size'] = self.embedding.embedding_dim\n",
    "        self.encoder = Rnn(param)\n",
    "\n",
    "        # outputs\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size*2, self.hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(param['dropout_p'])\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        questions = batch[0]\n",
    "\n",
    "        # mask\n",
    "        def get_mask(tensor): return torch.ne(tensor, 0)\n",
    "        question_mask = get_mask(questions)\n",
    "\n",
    "        # embedding\n",
    "        question_vec = self.embedding(questions)\n",
    "        question_vec = question_vec.transpose(0, 1)\n",
    "\n",
    "        # encoder (seq_len, batch_size, h*2)\n",
    "        question_vec = self.encoder(question_vec, question_mask)\n",
    "\n",
    "        # output\n",
    "        question_vec = torch.sum(question_vec, dim=0)\n",
    "        question_mask = question_mask.long().sum(1)\n",
    "        question_mask = question_mask.view(-1, 1).float()\n",
    "        question_vec = question_vec / question_mask  # (batch_size, h*2)\n",
    "\n",
    "        question_vec = self.dropout(question_vec)\n",
    "        output = self.fc1(question_vec)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)  # (batch_size, 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = config_model_rnn_1\n",
    "param['embedding'] = embedding\n",
    "\n",
    "model_lstm = Model_Rnn(param)\n",
    "model_lstm = model_lstm.cuda()\n",
    "model_best_state = None\n",
    "loss_best = 999\n",
    "accuracy_best = 0\n",
    "lr = LR\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer_param = filter(lambda p: p.requires_grad, model_lstm.parameters())\n",
    "optimizer = optim.Adam(optimizer_param, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training, param_num:588301\n"
     ]
    }
   ],
   "source": [
    "model_param_num = 0\n",
    "for parameter in model_lstm.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        model_param_num += parameter.nelement()\n",
    "print('start training, param_num:%d' % model_param_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training, epochs: 0, steps:459/4591, train_loss:0.2241, val_loss:0.1398, accuracy:0.9465, lr:0.0050, time:  15s\n",
      "training, epochs: 0, steps:918/4591, train_loss:0.1329, val_loss:0.1279, accuracy:0.9495, lr:0.0050, time:  32s\n",
      "training, epochs: 0, steps:1377/4591, train_loss:0.1271, val_loss:0.1236, accuracy:0.9510, lr:0.0050, time:  47s\n",
      "training, epochs: 0, steps:1836/4591, train_loss:0.1258, val_loss:0.1212, accuracy:0.9517, lr:0.0050, time:  63s\n",
      "training, epochs: 0, steps:2295/4591, train_loss:0.1223, val_loss:0.1228, accuracy:0.9503, lr:0.0050, time:  78s\n",
      "training, epochs: 0, steps:2754/4591, train_loss:0.1256, val_loss:0.1185, accuracy:0.9541, lr:0.0025, time:  94s\n",
      "training, epochs: 0, steps:3213/4591, train_loss:0.1186, val_loss:0.1118, accuracy:0.9552, lr:0.0025, time: 110s\n",
      "training, epochs: 0, steps:3672/4591, train_loss:0.1140, val_loss:0.1113, accuracy:0.9566, lr:0.0025, time: 127s\n",
      "training, epochs: 0, steps:4131/4591, train_loss:0.1137, val_loss:0.1066, accuracy:0.9584, lr:0.0025, time: 144s\n",
      "training, epochs: 0, steps:4590/4591, train_loss:0.1088, val_loss:0.1093, accuracy:0.9578, lr:0.0025, time: 159s\n",
      "training, best_loss:0.1066, best_accuracy:0.9584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model_Rnn(\n",
       "  (embedding): Embedding(\n",
       "    (embedding_fix): Embedding(200421, 300, padding_idx=0)\n",
       "    (embedding_v): Embedding(2, 300, padding_idx=0)\n",
       "  )\n",
       "  (encoder): Rnn(\n",
       "    (rnn): LSTM(300, 150, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=150, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = 0\n",
    "train_c = 0\n",
    "t_nums = len(train_questions) // batch_size\n",
    "every_nums = t_nums // 10\n",
    "time0 = time.time()\n",
    "loss_val_last = 99999.0\n",
    "for e in range(epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = [b.cuda() for b in batch]\n",
    "        model_lstm.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lstm(batch)\n",
    "        loss_value = criterion(outputs, batch[1].view(-1, 1).float())\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss_value.item()\n",
    "        train_c += 1\n",
    "\n",
    "        if train_c % every_nums == 0:\n",
    "            val_loss = 0\n",
    "            val_c = 0\n",
    "            correct_num = 0\n",
    "            sum_num = 0\n",
    "            with torch.no_grad():\n",
    "                model_lstm.eval()\n",
    "                for val_batch in val_loader:\n",
    "                    val_batch = [b.cuda() for b in val_batch]\n",
    "                    outputs = model_lstm(val_batch)\n",
    "                    loss_value = criterion(outputs, val_batch[1].view(-1, 1).float())\n",
    "\n",
    "                    correct_num += ((outputs > 0.5).long() == val_batch[1].view(-1, 1)).sum().item()\n",
    "                    sum_num += outputs.size(0)\n",
    "\n",
    "                    val_loss += loss_value.item()\n",
    "                    val_c += 1\n",
    "            print('training, epochs:%2d, steps:%2d/%2d, train_loss:%.4f, val_loss:%.4f, accuracy:%.4f, lr:%.4f, time:%4ds' %\n",
    "                      (e, (i+1), t_nums, train_loss/train_c, val_loss/val_c, correct_num/sum_num, lr, time.time()-time0))\n",
    "\n",
    "            train_loss = 0\n",
    "            train_c = 0\n",
    "\n",
    "            if loss_best > (val_loss / val_c):\n",
    "                accuracy_best = correct_num/sum_num\n",
    "                loss_best = val_loss / val_c\n",
    "                model_best_state = copy.deepcopy(model_lstm.state_dict())\n",
    "\n",
    "            # 动态调整lr\n",
    "            if loss_val_last < (val_loss / val_c) and lr >= 1e-4:\n",
    "                lr = lr * lr_decay\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "            loss_val_last = val_loss / val_c\n",
    "\n",
    "print('training, best_loss:%.4f, best_accuracy:%.4f' % (loss_best, accuracy_best))\n",
    "\n",
    "model_lstm.load_state_dict(model_best_state)\n",
    "model_lstm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Bi-gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_ensemble:\n",
    "    param = config_model_rnn_2\n",
    "    param['embedding'] = embedding\n",
    "\n",
    "    model_gru = Model_Rnn(param)\n",
    "    model_gru = model_gru.cuda()\n",
    "    model_best_state = None\n",
    "    loss_best = 999\n",
    "    accuracy_best = 0\n",
    "    lr = LR\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer_param = filter(lambda p: p.requires_grad, model_gru.parameters())\n",
    "    optimizer = optim.Adam(optimizer_param, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_ensemble:\n",
    "    model_param_num = 0\n",
    "    for parameter in model_gru.parameters():\n",
    "        if parameter.requires_grad:\n",
    "            model_param_num += parameter.nelement()\n",
    "    print('start training, param_num:%d' % model_param_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_ensemble:\n",
    "    train_loss = 0\n",
    "    train_c = 0\n",
    "    t_nums = len(train_questions) // batch_size\n",
    "    every_nums = t_nums // 10\n",
    "    time0 = time.time()\n",
    "    loss_val_last = 99999.0\n",
    "    for e in range(epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch = [b.cuda() for b in batch]\n",
    "            model_gru.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_gru(batch)\n",
    "            loss_value = criterion(outputs, batch[1].view(-1, 1).float())\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss_value.item()\n",
    "            train_c += 1\n",
    "\n",
    "            if train_c % every_nums == 0:\n",
    "                val_loss = 0\n",
    "                val_c = 0\n",
    "                correct_num = 0\n",
    "                sum_num = 0\n",
    "                with torch.no_grad():\n",
    "                    model_gru.eval()\n",
    "                    for val_batch in val_loader:\n",
    "                        val_batch = [b.cuda() for b in val_batch]\n",
    "                        outputs = model_gru(val_batch)\n",
    "                        loss_value = criterion(outputs, val_batch[1].view(-1, 1).float())\n",
    "\n",
    "                        correct_num += ((outputs > 0.5).long() == val_batch[1].view(-1, 1)).sum().item()\n",
    "                        sum_num += outputs.size(0)\n",
    "\n",
    "                        val_loss += loss_value.item()\n",
    "                        val_c += 1\n",
    "                print('training, epochs:%2d, steps:%2d/%2d, train_loss:%.4f, val_loss:%.4f, accuracy:%.4f, lr:%.4f, time:%4ds' %\n",
    "                          (e, (i+1), t_nums, train_loss/train_c, val_loss/val_c, correct_num/sum_num, lr, time.time()-time0))\n",
    "\n",
    "                train_loss = 0\n",
    "                train_c = 0\n",
    "\n",
    "                if loss_best > (val_loss / val_c):\n",
    "                    accuracy_best = correct_num/sum_num\n",
    "                    loss_best = val_loss / val_c\n",
    "                    model_best_state = copy.deepcopy(model_gru.state_dict())\n",
    "\n",
    "                # 动态调整lr\n",
    "                if loss_val_last < (val_loss / val_c) and lr >= 1e-4:\n",
    "                    lr = lr * lr_decay\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr\n",
    "\n",
    "                loss_val_last = val_loss / val_c\n",
    "\n",
    "    print('training, best_loss:%.4f， best_accuracy:%.4f' % (loss_best, correct_num/sum_num))\n",
    "\n",
    "    model_gru.load_state_dict(model_best_state)\n",
    "    print(model_gru.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 均值\n",
    "def ensemble_mean(model_result):\n",
    "    y_pred = np.zeros(shape=[len(model_result[0])])\n",
    "    for r in model_result:\n",
    "        y_pred += np.array(r)\n",
    "    y_pred = y_pred / len(model_result)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 阈值选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_loader = get_dataloader(\n",
    "    dataset=[val_questions, val_targets],\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score choosing, score:0.10, accuracy:0.5566\n",
      "score choosing, score:0.11, accuracy:0.5643\n",
      "score choosing, score:0.12, accuracy:0.5719\n",
      "score choosing, score:0.13, accuracy:0.5790\n",
      "score choosing, score:0.14, accuracy:0.5859\n",
      "score choosing, score:0.15, accuracy:0.5918\n",
      "score choosing, score:0.16, accuracy:0.5975\n",
      "score choosing, score:0.17, accuracy:0.6026\n",
      "score choosing, score:0.18, accuracy:0.6076\n",
      "score choosing, score:0.19, accuracy:0.6109\n",
      "score choosing, score:0.20, accuracy:0.6148\n",
      "score choosing, score:0.21, accuracy:0.6203\n",
      "score choosing, score:0.22, accuracy:0.6238\n",
      "score choosing, score:0.23, accuracy:0.6270\n",
      "score choosing, score:0.24, accuracy:0.6295\n",
      "score choosing, score:0.25, accuracy:0.6319\n",
      "score choosing, score:0.26, accuracy:0.6342\n",
      "score choosing, score:0.27, accuracy:0.6358\n",
      "score choosing, score:0.28, accuracy:0.6381\n",
      "score choosing, score:0.29, accuracy:0.6403\n",
      "score choosing, score:0.30, accuracy:0.6426\n",
      "score choosing, score:0.31, accuracy:0.6445\n",
      "score choosing, score:0.32, accuracy:0.6460\n",
      "score choosing, score:0.33, accuracy:0.6480\n",
      "score choosing, score:0.34, accuracy:0.6493\n",
      "score choosing, score:0.35, accuracy:0.6505\n",
      "score choosing, score:0.36, accuracy:0.6513\n",
      "score choosing, score:0.37, accuracy:0.6506\n",
      "score choosing, score:0.38, accuracy:0.6514\n",
      "score choosing, score:0.39, accuracy:0.6520\n",
      "score choosing, score:0.40, accuracy:0.6524\n",
      "score choosing, score:0.41, accuracy:0.6516\n",
      "score choosing, score:0.42, accuracy:0.6516\n",
      "score choosing, score:0.43, accuracy:0.6510\n",
      "score choosing, score:0.44, accuracy:0.6507\n",
      "score choosing, score:0.45, accuracy:0.6492\n",
      "score choosing, score:0.46, accuracy:0.6480\n",
      "score choosing, score:0.47, accuracy:0.6459\n",
      "score choosing, score:0.48, accuracy:0.6427\n",
      "score choosing, score:0.49, accuracy:0.6395\n",
      "score choosing, score:0.50, accuracy:0.6362\n",
      "valing, best_score:0.40, best_accuracy:0.6524\n"
     ]
    }
   ],
   "source": [
    "if is_ensemble is False:\n",
    "    scores = np.arange(0.1, 0.501, 0.01)\n",
    "    best_score = -1\n",
    "    best_accuracy = 0\n",
    "    for score in scores:\n",
    "        y_true_tmp = []\n",
    "        y_pred_tmp = []\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                val_batch = [b.cuda() for b in val_batch]\n",
    "                outputs = model_lstm(val_batch)\n",
    "                y_pred_tmp += (outputs > score).long().view(-1).cpu().numpy().tolist()\n",
    "                y_true_tmp += val_batch[1].view(-1).cpu().numpy().tolist()\n",
    "        acc_tmp = metrics.f1_score(y_true_tmp, y_pred_tmp)\n",
    "        print('score choosing, score:%.2f, accuracy:%.4f' % (score, acc_tmp))\n",
    "        if best_accuracy < acc_tmp:\n",
    "            best_score = score\n",
    "            best_accuracy = acc_tmp\n",
    "    print('valing, best_score:%.2f, best_accuracy:%.4f' % (best_score, best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_ensemble:\n",
    "    model_group = [model_lstm, model_gru]\n",
    "    scores = np.arange(0.1, 0.501, 0.01)\n",
    "    best_score = -1\n",
    "    best_accuracy = 0\n",
    "    for score in scores:\n",
    "        model_result = [[] for _ in range(len(model_group))]\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                val_batch = [b.cuda() for b in val_batch]\n",
    "\n",
    "                for index in range(len(model_group)):\n",
    "                    outputs = model_group[index](val_batch)\n",
    "                    outputs = outputs.view(-1).cpu().numpy().tolist()\n",
    "                    model_result[index] += outputs\n",
    "\n",
    "                y_true += val_batch[1].view(-1).cpu().numpy().tolist()\n",
    "\n",
    "        # 集成策略：均值\n",
    "        y_pred = ensemble_mean(model_result)\n",
    "\n",
    "        y_pred = (y_pred > score).astype(int).tolist()\n",
    "        acc_tmp = metrics.f1_score(y_true, y_pred)\n",
    "        print('score choosing, score:%.2f, accuracy:%.4f' % (score, acc_tmp))\n",
    "        if best_accuracy < acc_tmp:\n",
    "            best_score = score\n",
    "            best_accuracy = acc_tmp\n",
    "    print('valing, best_score:%.2f, best_accuracy:%.4f' % (best_score, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 测试模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = model_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_ensemble is False:\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            test_batch = [b.cuda() for b in test_batch]\n",
    "            outputs = model(test_batch)\n",
    "            outputs = (outputs > best_score).long()\n",
    "            result += outputs.view(-1).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if is_ensemble:\n",
    "    result = []\n",
    "    model_result = [[] for _ in range(len(model_group))]\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_loader:\n",
    "            test_batch = [b.cuda() for b in test_batch]\n",
    "\n",
    "            for index in range(len(model_group)):\n",
    "                outputs = model_group[index](test_batch)\n",
    "                outputs = outputs.view(-1).cpu().numpy().tolist()\n",
    "                model_result[index] += outputs\n",
    "\n",
    "        # 集成策略：均值\n",
    "        y_pred = ensemble_mean(model_result)\n",
    "\n",
    "        y_pred = (y_pred > best_score).astype(int).tolist()\n",
    "        result = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_file)\n",
    "submission = pd.DataFrame(\n",
    "    {'qid': test_df['qid'], 'prediction': result},\n",
    "    columns=['qid', 'prediction']\n",
    ")\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
